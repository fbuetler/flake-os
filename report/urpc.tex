\chapter{User-level Message Passing}

Communication between cores happens over shared memory. When a core is booted,
the parent core allocates some memory, maps it into its own address space and
provides its physical memory address and size to the child core. The child core
also maps this memory region into its address space, and now both core can
access the same memory region.

With this shared memory, both cores, need to settle on a common communication
protocol such that they understand each other.

\section{Communication Protocol}

Each core can acts as a server or a client. That means it may react on incoming
messages or initiate messages to other cores.  We decided on having a
consumer-producer queue mechanism to exchange messages between two cores, with
the shared memory having the size of two base pages. 

We first implemented the communication protocol with a shared memory size of one
base page, but it turned out that messages, that are sent to a core as a
response to a request and messages that are sent to a core as a request, may
interleave. So we settled to have a shared memory of the size of two base pages
such that one base page can be used for server communication and one for client
communication. 

However, the communication protocol for both server and client is the same. We
will now explain, how two cores can communicate with each other over shared
memory of the size of one base page.

As already mentioned, the communication protocol is based on a producer/consumer
queue implemented with as a circular ring buffer. We need exactly two ring
buffers to communicate, as one ring buffer is for sending a message from on
core, and receiving on the other core. The second ring buffer is the same but
vice-versa, such that both cores can both send and receive messages. 

That means we need to split the base page once again into two equals parts. The
parent core will use the first half of the base page for sending messages and
the second half of the base page for receiving messages. The child core does the
same, but receiving on the first half and sending on the second half. Both cores
keep a pointer to an offset in the ring buffer to know where they should expect
the next message.

\section{Message Format}

Each message in our communication protocol is exactly 64 bytes, i.e. the size of
a cache line. That is important, such that we don't need to flush the message to
the memory, but it is enough to have the message in the cache, for the other
core to see it. This gives us a huge performance gain.

Each message has a header and a payload. The header contains metadata such as
the message type, the message state, payload size and if it is the last messages
in a chain of fragmented messages. The latter is important for sending messages
that are larger than 64 bytes, but more on that later. The header has the size
of exactly 3 bytes, and therefore there is space for 61 bytes of payload for
each message.

The most interesting header parameter is the message state. A message can have
three different states: created, sent or received. When a message is initially
created, it is marked as sent. When a message is sent from the sender's
perspective it is marked as sent, and it is marked as received from the
receiver, when it consumed the whole message.

\section{Sending and Receiving Messages}

To send a message, we need to get our current offset into the ring buffer, i.e.
a particular cache line. First, we check if the previously sent message over
that cache line is marked as sent in its header. If that is not the case, then
we filled the circular ring buffer completely and cannot send another message
until the receiver has consumed some messages. However, if the message is not
marked as sent, but as received, then we can use this cache line to transmit
another message. We copy the message into the cache line, mark it in its header
as sent, and update our offset into the ring buffer to point to the next cache
line. The tricky part here is to deal with ARM's weak memory model. A weak
memory model does not guarantee that stores from one core are observer by
another core in the same order, that they are stored by the former. Luckily,
there are barriers to enforce exactly that.

We need a barrier between checking for the message state and copying the message
in the cache line to ensure that we first check the state being received before
we overwrite possibly not yet consumed messages. And we also need a barrier
after copying the message into the cache line and marking the message as sent,
such that the message is actually written to memory before logically marked as
sent and consumed by the receiver.

To receive a message, it computes the current offset into the ring buffer to
know in which cache line the receiver should expect the next message. It  checks
repeatedly the message stored in this cache line for its message state. Once,
the message state is marked as sent, the receiver got a new message to be
consumed. To consume it, it is copying the content from the cache line, marks
the message as received and updates its offset into the ring buffer.

Again, we need barriers to make the whole receiving process work. Similarly, to
the sender, we need to insert a barrier before consuming a message such, that we
are sure that we first check the message state to be marked as sent before
actually reading its content. Also, we need another barrier between copying the
message content from the cache line and marking the message as received. The
same reasoning as above applies here.

\section{Sending large messages}

At this point, two core are capable of communicating with each other. But only
if the message, they would like to send, fit into a single cache line. We went
the extra mile and implemented the extra challenge fragmentation and reassembly.
For this we need to split the payload, that should be sent, into chunks fitting
into a cache line and reassemble them on the receiver side. Sending chunked
messages is pretty straight forward, but one point to mention here is the case
when we send many chunked messages and completely fill the ring buffer and hence
fail sending the remaining message. In this case, we implemented a linear back
off functionality that retries send the messages up to 32 times, such that the
receiver has time to consume messages and give space for sending the remaining
messages.

We decided to limit the maximal allowed size of messages sent over UMP to two
base page sizes. This way, we can allocate a temporary buffer on the receiver
side, that is guaranteed to contain the whole message after reassembling. The
receiver then simply needs to read all chunked message and reassemble them.

\section{Synchronous communication}

To provide synchronous communication over UMP we also added a function to our
interface that, sends a message and immediately tries to receive one, such that
we could offload this functionality directly into the library.

% maybe talk about server/client and binding
