\chapter{Memory Allocator}

\section{Physical Frame Allocator}

\subsection{Algorithm}

We first wanted to implement a buddy allocator and implemented a reference
implementation for testing purposes outside of Barrelfish and then imported that
into Barrelfish as our memory allocator. Unfortunately, we struggled with understanding
the concept of how capabilities are split, i.e. while we understood that we get a new capref, 
it was not clear for us what happens to the rest. Do we get another new capref
with that? After some time, we figured out that it the original capability is
still untouched from the point of view of the memory allocator and the only
difference is that when we try to split the same part, we get an error from the
underlying capability system.

At first we further had neither decided on how to handle the non-continuous memory regions that are
handed to the memory allocator, nor how to handle memory regions that are not a
power of two in size.

All this then led us to implement a simple first fit allocator with a doubly
linked list to get things started. Later, we improved the first fit allocator to a
next fit allocator with a circular buffer. Since in a circular buffer, we can
move the head to a block which contains a left-over block
from a previous use of the allocator with higher probability, this heavily improves search-performance for 
a new block.

However, there were more obstacles to overcome: At first we were puzzled with how to allocate an
object on the heap (i.e. using malloc in normal C programs). We then
figured out that this is the purpose of the slab allocator and used it for 
storing the memory.

As we needed a list implementation for our first attempt of implementing the
memory allocator, we added our own to the source tree. Little did we know that
there is already an implementation under collections/. However, this one 
uses malloc, making it of little use at this point in time.
We later changed the memory allocator to natively be implemented with links, so we
didn't need our list implementation anymore.

In the following, we are going to discuss our implementation of a next fit memory
allocator.


\subsection{Datastructure}\label{subsec:mm-tracker}

As mentioned above, the data structure for the next fit allocator is a circular
buffer.  The circular buffer is implemented as a doubly linked list and the
smallest unit is a \texttt{mmnode} (memory manager node).  Each \texttt{mmnode} represents
a continuous region of memory and is linked to the next node that is represented the
next closest region of memory. The memory nodes that represent the start (lowest
memory address) and the end (highest memory address) of the memory are linked
with each other.  A node can be split into two nodes to accommodate its size to
the size of the requested memory or be merged with one of its neighbors, if
they are free.  A \texttt{mmnode} contains its type, whether it is free or allocated,
pointers to its adjacent neighbors, its memory base address, its size and a
capinfo. The capinfo contains the capref and its original memory base address
and size:

\begin{minted}{c}
enum nodetype { NodeType_Free, NodeType_Allocated };

struct capinfo {
    struct capref cap;
    genpaddr_t base;
    size_t size;
};

typedef struct mmnode_t {
    enum nodetype type;
    struct capinfo capinfo;
    struct mmnode_t *prev;
    struct mmnode_t *next;
    genpaddr_t base;
    gensize_t size;
} mmnode_t;
\end{minted}

The data structure is concluded by storing a pointer to the head of the linked
list. As it is a circular buffer, it does not really have a natural head, but
this is the pointer to the current position in the circular buffer. The memory
allocator has more to it and they will be discussed at a later point.

\begin{minted}{c}
struct mm {
    struct slab_allocator slab_allocator;
    slot_alloc_t slot_alloc;
    slot_refill_t slot_refill;
    void *slot_allocator;
    enum objtype objtype;

    mmnode_t *head;
};
\end{minted}

\subsection{Add memory}

Initially, the memory allocators head is \mintinline{c}{NULL} as there is no memory avilable at that
time. The memory allocator gets hold of new memory from the init
process that reads that information from the bootinfo. With each new memory
region added, we add another node representing this region. The memory region
comes in the form of a capref to a RAM capability. By inspecting it, we can read
its memory base address and size and create a node for it. As it is our first node,
its next and prev pointer point to itself and the head points to that.  As more
memory regions are added, more nodes are created and inserted before the current
head. We mark all these nodes as free memory.

\subsection{Allocate memory}

If any other component in Barrelfish wants a ram capability, it has to request it of us.
After some initial sanity checks of the request, we now need to find a memory
region that fits the request. There are multiple parameters such as size of the
memory and its alignment, i.e. it has to start on an address that is a multiple of
the alignment.  To fulfill these requirements, we traverse the circular buffer
until we find a node that is marked as free and its size is large enough. If the
memory that is represented by the node is not aligned as requested, we check if
the aligned version still fits in this node and continue our traversal
otherwise. If the node is not aligned but is still big enough, we split the
node at an aligned position into two nodes with the right one being aligned.
Splitting just inserts the new node into the circular buffer by pointing the
previous node to itself and itself to the next node. Of course we need to update
the base addresses and sizes of the nodes.  As we now have a node that fulfills
all requirements, we retype the capref to the correct size for both nodes and
store it as the result.  This is also the node we are going to start our
search from for future memory allocation request (i.e. next fit).

\subsection{Free memory}

Freeing memory reverses an allocation. We identify the correct node by
inspecting the provided capref, that gives us the base address and size.  If we
did not find the node we did not hand out this memory region and ignore the free
request. If we found the node, we destroy the capref and mark the node as
free. Further, we check both neighbors for a possible node merge. In a node
merge, we check whether they represent physical adjacent memory regions to
our memory region. This is crucial to not try and merge the "first" and "last"
node, as our data structure is a circular buffer but the memory is still linear.
Further, the neighbors have to be marked free as well. If all that is the case, we
merge them by simply pointing the previous node to the next node.

This concludes our memory allocator.

\subsection{Slab Memory Allocator}
Our mmnodes need some memory as well to store their own data. But where to take it from,
when the nodes have the be created first before memory is handed out by the
memory allocator? This is the purpose of the slab allocator. It can hand out
memory but only of a certain block size. The slab allocator instance used in
the memory allocator has a block size of the size of a mmnode. We initially give
the slab allocator some memory from the init process and if that runs out we
have to get it from the actual memory allocator that should now be bootstrapped.
The slab allocator is used in a node split to create a new node and also when we
add some memory to the memory allocator. We return a slab on a node merge. We
also need to periodically check whether we need to refill the slab allocator
with new memory. We do this after each memory allocation in our memory
allocator.  Why do we even need to do that?  Well, once the slab allocator has
run out of memory and only then want to allocate more memory to it we need to
ask the memory allocator for memory, but this one needs a slab to allocate
memory. This is a problem, as we can't handle such recursive memory allocation requests.
Thus, we use the amount of remaining free slabs in our datastructure 
as a tie-breaker to decide if we want to allocate new slabs or handle a memory request.

\subsection{Slot Memory Allocator}
To store the capref that we hand out for every memory allocation we need some memory
as well. Initially it also gets some memory from the init process. We allocate
a slot for each capref we return for an allocation request and free the slot on
a free request.  This one also needs to be refilled and we check that
before each slot allocation. This one is even more tricky, if we run out of
slots. A slot refill needs to allocate memory, that needs a slab for its
bookkeeping that may trigger another slab refill which then requires a slot as well. For
this purpose, we also added a function that reports the number of free slots,
so we can perform the necessary checks.

\section{Frame Mapping}

\subsection{Page Table}
Next, we need to be able to map a virtual address to a frame capability that was
derived from a RAM capability, which represents a physical memory region.  For
this purpose, we need to create a page table that is able to do this mapping.  We
have a 4 level deep page table, which essentially copies the actual page table
used by virtual-to-physical address translation in hardware.
Thus, our page table is a tree consisting of nodes which each represent 
an L0, L1, L2 or L3 page table with the corresponding 512 entries of it.
Each entry either is a pointer to a node of the next level or contains the 
mapping of a single page mapped to the given virtual address given by the 
indexing into L0, L1, L2 and L3.
The first level (L0) is at a well known location and exists already. 
So we only need to create the other levels on the fly as we
need them, i.e. need to store a value in one of its entries.  Once we reach the
last level, we store the frame capref/page table mapping and are done. 
The memory to store the data structure is once again done by a slab allocator (not the same one) with some memory provided by the init process.

\subsection{Slab Allocator Refills}

Now, we are able to actually refill the slab allocator. Here we first allocate
a frame capref (derived from a RAM capref) and map the frame into the page table
for the given virtual address. We can then tell the slab allocator that there is
some more memory at the virtual address for its use.

\section{Tests}
To test all the above discussed functionalities we wrote some tests. They include
alternating memory allocations and frees of one base page i.e. 4 KiB (the
iterations may vary between 8 and 512 and alignment of 1 or 4 KiB), consecutive
memory allocation and then frees (again, different parameters), provoking many
node merges by first allocating many regions and then free every second and
finally the rest. There are also tests for frame mappings, slot and slab
allocator refills and a test where the requested size is exponentially increased
until there is no memory left to fulfill the request, and then freeing all
memory.  During this, there are page tables created that are not freed at the moment.

\section{Challenges}

The implementation of the memory allocator needed to overcome many
challenges. As said, the retyping of capabilities wasn't very clear in the beginning
and more information about the capability management system was needed to correctly
implement our bulk allocator. As a first real milestone, it also served as a
big refresher on basic pointer arithmetic, which led to hard-to-debug errors in the code.

However, what was the most work was to successfully implement slab and slot refills
without causing further recursive problems. While the theory behind is easy, it is much harder
to implement in practice. While recursive allocation is the problem we initially wanted
to solve, recursive refilling is a problem which was introduced. With some more locking
during refilling processes, even that problem has been solved.

\section{Outlook}

The circular doubly-linked-list next-best-fit allocator has been working quite stable
for the rest of the problem. What still would have been interesting is to implement
a buddy allocator. Even though the memory allocator is a bulk allocator, it is used very often in an operating system. Thus, optimizing it can be very beneficial.